{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://github.com/jon-chun/sentimentarcs_simplified/blob/main/sentimentarcs_supersimplified.ipynb\" target=\"_blank\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/>\n",
        "</a>\n"
      ],
      "metadata": {
        "id": "VwidmsGf0J_Z"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwxnxnOXF4TB"
      },
      "source": [
        "# SentimentArcs Simplified Notebook\n",
        "## ***(Version 20241214)***\n",
        "\n",
        "## ***Use GPU for Transformer Models***\n",
        "\n",
        "* 28 Oct 2022: Created\n",
        "* 12 Nov 2024: Last Major Update\n",
        "* Jon Chun\n",
        "\n",
        "A simplified version of SentimentArcs Notebooks with only the most common and state-of-the-art sentiment analysis models in the ensemble. This operationalizes SentimentArcs more accessible and robust.\n",
        "\n",
        "* https://github.com/jon-chun/sentimentarcs_notebooks\n",
        "\n",
        "* https://arxiv.org/pdf/2110.09454.pdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: Your uploaded text files should have 3 parts separated by two '_' underscores\n",
        "# \"book\" + \"_\" + \"title-words-separated-by-hyphens\" + \"_\" + \"author\" (where author is one word with only alphanumeric chars, no hypens or other punctuations)\n"
      ],
      "metadata": {
        "id": "z8apG_y4zrzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%whos\n",
        "\n",
        "# CHECK program space for imported and defined objects"
      ],
      "metadata": {
        "id": "ou45nK0J-SAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CONFIGURE: If True, use one global window value\n",
        "#            If False, allow individual custom values\n",
        "\n",
        "GLOBAL_WIN_FLAG = True\n",
        "GLOBAL_WIN = 0.05  # If True, percent of book length (e.g. 0.1 = 10%)\n",
        "\n",
        "# GLOBAL_Win = False"
      ],
      "metadata": {
        "id": "9-iFT875ow1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST: To avoid later permission pop-up interruption\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "hf_key = userdata.get('HF_TOKEN')\n",
        "\n",
        "if hf_key is None:\n",
        "  print(\"HF_KEY not found in secrets\")\n",
        "else:\n",
        "  os.environ['HF_KEY'] = hf_key\n",
        "  print(\"HF_KEY found and set\")\n"
      ],
      "metadata": {
        "id": "HjC0011ing55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST: To avoid later permission pop-up interruption\n",
        "\n",
        "with open('test.txt', 'w') as f:\n",
        "  f.write(\"test content\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download('test.txt')"
      ],
      "metadata": {
        "id": "VELfD2hFnlND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvpU9suaP0bA"
      },
      "source": [
        "# **PRE-REQUISITES**\n",
        "\n",
        "## INPUT Text File\n",
        "- name: format 'book_{title-words}_{author}.txt'\n",
        "-- eg book_animal-farm_gorwell.txt\n",
        "-- eg book_the-great-gatsby_fscottfitzgerald.txt\n",
        "- content: cleaned plain text\n",
        "\n",
        "## API Keys Necessary\n",
        "- huggingface.co ('HF_TOKEN')\n",
        "- wandb.com ('WANDB_TOKEN')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84IZ26RpiS7f"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVKek_uU7NG_"
      },
      "source": [
        "## Install Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dd4CiRWNof2-"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers[sentencepiece]\n",
        "\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Installed by default in Colab\n",
        "\n",
        "# !pip install nltk"
      ],
      "metadata": {
        "id": "D3KGaqV4UCDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "2v0UfOLDUB7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -U spacy"
      ],
      "metadata": {
        "id": "XpDYrDXrTEkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "ihGZIzVoTFqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install pysbd  # Python Sentence Boundry Detection"
      ],
      "metadata": {
        "id": "EEepL8nbcXRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Installed by default in Colab\n",
        "\n",
        "# !pip install TextBlob"
      ],
      "metadata": {
        "id": "AfxevHnS5sUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vaderSentiment"
      ],
      "metadata": {
        "id": "JBWNcISNckt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEHDAX84QeDg"
      },
      "outputs": [],
      "source": [
        "# May require [RESET RUNTIME]\n",
        "\n",
        "# !pip install modin[all]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6AarbSqQweP"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7asUI7bIT0Zr"
      },
      "outputs": [],
      "source": [
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vBv9P-fZaxS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "# import modin.pandas as pd_modin\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from scipy.stats import zscore\n",
        "from scipy.signal import find_peaks, argrelextrema\n",
        "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
        "import textwrap\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "import re\n",
        "import os\n",
        "\n",
        "import chardet  # Library for detecting encoding\n",
        "\n",
        "from tqdm import tqdm\n",
        "import tqdm.notebook as tq\n",
        "# for i in tq.tqdm(...):"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import pysbd"
      ],
      "metadata": {
        "id": "tWmKzPtsRLCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import spacy"
      ],
      "metadata": {
        "id": "2pSNCMa-T0Nz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YGBQuOQ8262"
      },
      "source": [
        "## Configure Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aex0daSTdhQs"
      },
      "outputs": [],
      "source": [
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdyvMBNkiYtk"
      },
      "source": [
        "# Global Variables & Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDRQt-CWditY"
      },
      "source": [
        "## Global Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qb_U-MrR-2x0"
      },
      "outputs": [],
      "source": [
        "# Main (Modin) DataFrame for Novel Sentiments\n",
        "\n",
        "sentiment_df = pd.DataFrame\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMhPyO0F85GJ"
      },
      "source": [
        "## Common Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmBgCGUCDYGA"
      },
      "outputs": [],
      "source": [
        "def verify_novel(novel_str, index_ends=500):\n",
        "  '''\n",
        "  INPUT: string in some stage of processing\n",
        "  OUTPUT: display summary index_ends chars of header/footer for verification\n",
        "  '''\n",
        "\n",
        "  print(f'Novel Name: {novel_name_str}')\n",
        "  print(f'  Char Len: {len(novel_str)}')\n",
        "  print('====================================\\n')\n",
        "  print(f'Beginning:\\n\\n {novel_str[:index_ends]}\\n\\n')\n",
        "  print('\\n------------------------------------')\n",
        "  print(f'Ending:\\n\\n {novel_str[-index_ends:]}\\n\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4284uzStHpyU"
      },
      "outputs": [],
      "source": [
        "def save_text2txt_and_download(text_obj, filename_base='BOOK_FILENAME_BASE', file_suffix='_save.txt'):\n",
        "  '''\n",
        "  INPUT: text object and suffix to add to output text filename\n",
        "  OUTPUT: Write text object to text file (both temp VM and download)\n",
        "  '''\n",
        "\n",
        "  if type(text_obj) == str:\n",
        "    print('STEP 1. Processing String Object\\n')\n",
        "    str_obj = text_obj\n",
        "  elif type(text_obj) == list:\n",
        "    if (len(text_obj) > 0):\n",
        "      if type(text_obj[0]) == str:\n",
        "        print('STEP 1. Processing List of Strings Object\\n')\n",
        "        str_obj = \"\\n\".join(text_obj)\n",
        "      else:\n",
        "        print('ERROR: Object is not an List of Strings [save_text2txt_and_download()]')\n",
        "        return -1\n",
        "    else:\n",
        "      print('ERROR: Object is an empty List [save_text2txt_and_download()]')\n",
        "      return -1\n",
        "  else:\n",
        "    print('ERROR: Object Type is neither String nor List [save_text2txt_and_download()]')\n",
        "    return -1\n",
        "\n",
        "  datetime_str = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
        "  # out_filename = novel_name_str.split('.')[0] + '_' + datetime_str + file_suffix\n",
        "  # out_filename = novel_name_str.split('.')[0] + file_suffix\n",
        "  out_filename = filename_base + file_suffix\n",
        "\n",
        "  # Write file to temporary VM filesystem\n",
        "  print(f'STEP 2. Saving textfile to temporary VM file: {out_filename}\\n')\n",
        "  with open(out_filename, \"w\") as fp:\n",
        "    fp.write(str_obj)\n",
        "\n",
        "  # Download permanent copy of file\n",
        "  print(f'STEP 3. Downloading permanent copy of textfile: {out_filename}\\n')\n",
        "  files.download(out_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcZY1AuoBUqd"
      },
      "outputs": [],
      "source": [
        "def save_plot2png(file_suffix='_plot.png', nodate=True, base_filename='plot'):\n",
        "    '''\n",
        "    Save the current plot to a PNG file.\n",
        "\n",
        "    INPUT:\n",
        "    - file_suffix: The suffix to add to the output PNG filename.\n",
        "    - nodate: If True, the date is not added to the filename; if False, the date is included.\n",
        "    - base_filename: The base filename to use (defaults to \"plot\" if unspecified).\n",
        "\n",
        "    OUTPUT:\n",
        "    - Saves the current plot to a PNG file in the temporary VM and downloads it.\n",
        "    '''\n",
        "    # Construct the filename based on whether the date should be included\n",
        "    datetime_str = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
        "    if nodate:\n",
        "        out_filename = base_filename + file_suffix\n",
        "    else:\n",
        "        out_filename = base_filename + '_' + datetime_str + file_suffix\n",
        "\n",
        "    # Save the plot\n",
        "    plt.savefig(out_filename)\n",
        "    print(f'STEP 1. Saving plot to temporary VM file: {out_filename}\\n')\n",
        "\n",
        "    # Download the saved file\n",
        "    print(f'STEP 2. Downloading permanent copy of the PNG file: {out_filename}\\n')\n",
        "    files.download(out_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CeA0VLNT56TZ"
      },
      "outputs": [],
      "source": [
        "def save_df2csv_and_download(df, file_suffix='_data.csv', nodate=True, base_filename='data'):\n",
        "    '''\n",
        "    Save the DataFrame to a CSV file.\n",
        "\n",
        "    INPUT:\n",
        "    - df: The DataFrame to save.\n",
        "    - file_suffix: The suffix to add to the output CSV filename.\n",
        "    - nodate: If True, the date is not added to the filename; if False, the date is included.\n",
        "    - base_filename: The base filename to use (defaults to \"data\" if unspecified).\n",
        "\n",
        "    OUTPUT:\n",
        "    - Saves the DataFrame to a CSV file in the temporary VM and downloads it.\n",
        "    '''\n",
        "    # Construct the filename based on whether the date should be included\n",
        "    datetime_str = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
        "    if nodate:\n",
        "        out_filename = base_filename + file_suffix\n",
        "    else:\n",
        "        out_filename = base_filename + '_' + datetime_str + file_suffix\n",
        "\n",
        "    # Save the DataFrame to CSV\n",
        "    df.to_csv(out_filename, index=False)\n",
        "    print(f'STEP 1. Saving DataFrame to temporary VM CSV file: {out_filename}\\n')\n",
        "\n",
        "    # Download the saved CSV file\n",
        "    print(f'STEP 2. Downloading permanent copy of the CSV file: {out_filename}\\n')\n",
        "    files.download(out_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqLNZq62HImu"
      },
      "source": [
        "# Get Clean Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YVNpxpABMoQ"
      },
      "source": [
        "### **[STOP to INPUT]** Upload Trimmed Gutenberg Text (no header/footer)\n",
        "\n",
        "Get plain text of familiar novel at:\n",
        "* https://gutenberg.net.au/ (AUS)\n",
        "* https://gutenberg.org/ (US)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# PART 1: Upload Plain Text File\n",
        "\n",
        "novel_name_str = ''\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Allows for multiple file uploads, will only process the last\n",
        "# Left in for future feature addition (processing multiple files at once)\n",
        "for fn in uploaded.keys():\n",
        "    print(f'User uploaded file \"{fn}\" with length {len(uploaded[fn])} bytes.')\n",
        "    novel_name_str = fn\n",
        "\n",
        "# Extract from Dict and detect encoding\n",
        "raw_data = uploaded[novel_name_str]  # Access the uploaded file's raw binary data\n",
        "\n",
        "# Step 1: Detect the encoding\n",
        "encoding_info = chardet.detect(raw_data)\n",
        "detected_encoding = encoding_info['encoding']\n",
        "print(f\"Detected file encoding: {detected_encoding}\")\n",
        "\n",
        "# Step 2: Attempt decoding with detected encoding\n",
        "try:\n",
        "    novel_raw_str = raw_data.decode(detected_encoding)\n",
        "except (UnicodeDecodeError, TypeError):\n",
        "    print(f\"Failed to decode using detected encoding: {detected_encoding}. Attempting fallback.\")\n",
        "\n",
        "    # Step 3: Fallback to a common encoding\n",
        "    try:\n",
        "        novel_raw_str = raw_data.decode('utf-8')\n",
        "        print(\"Successfully decoded using utf-8 as fallback.\")\n",
        "    except UnicodeDecodeError:\n",
        "        try:\n",
        "            novel_raw_str = raw_data.decode('latin1')  # Latin1 as a last resort\n",
        "            print(\"Successfully decoded using latin1 as fallback.\")\n",
        "        except UnicodeDecodeError as e:\n",
        "            print(f\"Critical error: Unable to decode file. Error: {e}\")\n",
        "            raise\n",
        "\n",
        "# If decoding succeeded, proceed with further processing\n",
        "print(f\"First 500 characters of the decoded text:\\n{novel_raw_str[:500]}\")\n",
        "''';"
      ],
      "metadata": {
        "id": "z-z5sGo9m_Ff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# URGENT ADD ERROR DETECTION:\n",
        "\n",
        "# NOTE: Your uploaded text files should have 3 parts separated by two '_' underscores\n",
        "# \"book\" + \"_\" + \"title-words-separated-by-hyphens\" + \"_\" + \"author\" (where author is one word with only alphanumeric chars, no hypens or other punctuations)\n"
      ],
      "metadata": {
        "id": "NT3xzHng1qht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "\n",
        "def extract_version_number(filename):\n",
        "    \"\"\"\n",
        "    Extracts version number from a filename with pattern ' (n).txt'\n",
        "    Returns (root_name, version_number) or (None, None) if no version found\n",
        "    \"\"\"\n",
        "    pattern = r'^(.+?) \\((\\d+)\\)\\.txt$'\n",
        "    match = re.match(pattern, filename)\n",
        "    if match:\n",
        "        return match.group(1), int(match.group(2))\n",
        "    return None, None\n",
        "\n",
        "def validate_upload_filename(filename_str):\n",
        "    \"\"\"\n",
        "    Validates filename and handles versioned files.\n",
        "    Returns (is_valid, violations, root_name, version_number)\n",
        "    \"\"\"\n",
        "    violations = []\n",
        "    root_name = None\n",
        "    version_number = None\n",
        "\n",
        "    # Check if this is a versioned file\n",
        "    root_name, version_number = extract_version_number(filename_str)\n",
        "\n",
        "    # If this is a versioned file, we'll validate the root name\n",
        "    if version_number is not None:\n",
        "        filename_to_validate = f\"{root_name}.txt\"\n",
        "    else:\n",
        "        filename_to_validate = filename_str\n",
        "\n",
        "    # Rule 1: Basic filename validation for OS compatibility\n",
        "    if ' ' in filename_to_validate:\n",
        "        violations.append(f\"Rule 1 violated: Filename '{filename_to_validate}' contains spaces\")\n",
        "\n",
        "    # Rule 4: Check .txt extension\n",
        "    if not filename_to_validate.endswith('.txt'):\n",
        "        violations.append(f\"Rule 4 violated: Filename '{filename_to_validate}' does not end with .txt extension\")\n",
        "        return False, violations, None, None\n",
        "\n",
        "    # Remove the .txt extension for further checks\n",
        "    name_without_ext = filename_to_validate[:-4]\n",
        "\n",
        "    # Rule 2: Check for exactly two underscores\n",
        "    underscore_count = name_without_ext.count('_')\n",
        "    if underscore_count != 2:\n",
        "        violations.append(f\"Rule 2 violated: Filename '{filename_to_validate}' has {underscore_count} underscores, exactly 2 required\")\n",
        "\n",
        "    # Check for consecutive underscores\n",
        "    if '__' in name_without_ext:\n",
        "        violations.append(f\"Rule 2 violated: Filename '{filename_to_validate}' contains consecutive underscores\")\n",
        "\n",
        "    # Rule 3 & 6: Check for valid characters in segments\n",
        "    segments = name_without_ext.split('_')\n",
        "    valid_segment_pattern = re.compile(r'^[a-zA-Z0-9\\-]+$')\n",
        "\n",
        "    if len(segments) == 3:\n",
        "        for i, segment in enumerate(segments, 1):\n",
        "            if not valid_segment_pattern.match(segment):\n",
        "                invalid_chars = set(char for char in segment if not (char.isalnum() or char == '-'))\n",
        "                if invalid_chars:\n",
        "                    violations.append(f\"Rules 3/6 violated: Segment {i} '{segment}' contains invalid characters: {', '.join(invalid_chars)}\")\n",
        "\n",
        "    # Rule 5: Check for whitespace (additional explicit check)\n",
        "    if any(char.isspace() for char in filename_to_validate):\n",
        "        violations.append(f\"Rule 5 violated: Filename '{filename_to_validate}' contains whitespace characters\")\n",
        "\n",
        "    return len(violations) == 0, violations, root_name, version_number\n",
        "\n",
        "def handle_file_versioning(original_filename, uploaded_data):\n",
        "    \"\"\"\n",
        "    Handles file versioning, backups, and renames.\n",
        "    Returns (success, final_filename, error_message)\n",
        "    \"\"\"\n",
        "    is_valid, violations, root_name, version_number = validate_upload_filename(original_filename)\n",
        "\n",
        "    if not is_valid:\n",
        "        return False, None, violations\n",
        "\n",
        "    # If this is a versioned file, we need to handle it specially\n",
        "    if version_number is not None:\n",
        "        base_filename = f\"{root_name}.txt\"\n",
        "        backup_filename = f\"{root_name}_bu{version_number}.txt\"\n",
        "\n",
        "        # If original file exists, create backup\n",
        "        if os.path.exists(base_filename):\n",
        "            try:\n",
        "                os.rename(base_filename, backup_filename)\n",
        "                print(f\"Created backup: {backup_filename}\")\n",
        "            except OSError as e:\n",
        "                return False, None, f\"Error creating backup: {str(e)}\"\n",
        "\n",
        "        # Save the new file with the base filename\n",
        "        try:\n",
        "            with open(base_filename, 'wb') as f:\n",
        "                f.write(uploaded_data)\n",
        "            print(f\"Saved new version as: {base_filename}\")\n",
        "            return True, base_filename, None\n",
        "        except OSError as e:\n",
        "            return False, None, f\"Error saving new file: {str(e)}\"\n",
        "\n",
        "    # For non-versioned files, just save directly\n",
        "    try:\n",
        "        with open(original_filename, 'wb') as f:\n",
        "            f.write(uploaded_data)\n",
        "        return True, original_filename, None\n",
        "    except OSError as e:\n",
        "        return False, None, f\"Error saving file: {str(e)}\"\n",
        "\n",
        "# PART 1: Upload Plain Text File with Validation\n",
        "novel_name_str = ''\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Process uploaded files\n",
        "for fn in uploaded.keys():\n",
        "    print(f'User uploaded file \"{fn}\" with length {len(uploaded[fn])} bytes')\n",
        "\n",
        "    success, final_filename, error = handle_file_versioning(fn, uploaded[fn])\n",
        "\n",
        "    if not success:\n",
        "        if isinstance(error, list):\n",
        "            print(f'\\nERROR: Invalid filename \"{fn}\"')\n",
        "            print(\"The following rules were violated:\")\n",
        "            for violation in error:\n",
        "                print(f\"- {violation}\")\n",
        "            print(\"\\nFilename rules:\")\n",
        "            print(\"1. Must be a valid filename under MacOS, Win 11 and Linux\")\n",
        "            print(\"2. Must have exactly TWO embedded underscores '_' in the filename root\")\n",
        "            print(\"3. May have hyphens '-' embedded within the 3 subtext segments\")\n",
        "            print(\"4. Must end with the '.txt' filename extension\")\n",
        "            print(\"5. Must have no embedded whitespaces\")\n",
        "            print(\"6. Must have no other embedded punctuation except for that allowed in rules 2-4\")\n",
        "            print(\"\\nExample of valid filename: author-name_book-title_2024.txt\")\n",
        "        else:\n",
        "            print(f\"Error processing file: {error}\")\n",
        "        continue\n",
        "\n",
        "    novel_name_str = final_filename\n",
        "\n",
        "    # Extract from Dict and detect encoding\n",
        "    raw_data = uploaded[fn]  # Use original filename to get from uploaded dict\n",
        "\n",
        "    # Step 1: Detect the encoding\n",
        "    encoding_info = chardet.detect(raw_data)\n",
        "    detected_encoding = encoding_info['encoding']\n",
        "    print(f\"Detected file encoding: {detected_encoding}\")\n",
        "\n",
        "    # Step 2: Attempt decoding with detected encoding\n",
        "    try:\n",
        "        novel_raw_str = raw_data.decode(detected_encoding)\n",
        "    except (UnicodeDecodeError, TypeError):\n",
        "        print(f\"Failed to decode using detected encoding: {detected_encoding}. Attempting fallback.\")\n",
        "        try:\n",
        "            novel_raw_str = raw_data.decode('utf-8')\n",
        "            print(\"Successfully decoded using utf-8 as fallback.\")\n",
        "        except UnicodeDecodeError:\n",
        "            try:\n",
        "                novel_raw_str = raw_data.decode('latin1')\n",
        "                print(\"Successfully decoded using latin1 as fallback.\")\n",
        "            except UnicodeDecodeError as e:\n",
        "                print(f\"Critical error: Unable to decode file. Error: {e}\")\n",
        "                raise\n",
        "\n",
        "    # If decoding succeeded, proceed with further processing\n",
        "    print(f\"First 500 characters of the decoded text:\\n{novel_raw_str[:500]}\")"
      ],
      "metadata": {
        "id": "doPZROh23QCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -altr"
      ],
      "metadata": {
        "id": "dqSs1Gt535rV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PART 2: Verify Upload Text\n",
        "\n",
        "verify_novel(novel_raw_str)"
      ],
      "metadata": {
        "id": "EmmySbnVYQCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PART 3: Verify FILENAME variants based upon uploaded filename\n",
        "\n",
        "BOOK_FILENAME_AUTHOR_CLEAN = novel_name_str.split('.')[0]\n",
        "print(f\"BOOK_FILENAME_AUTHOR_CLEAN: {BOOK_FILENAME_AUTHOR_CLEAN}\")\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "# prompt: set book_filename_author_clean = a cleaned version of BOOK_FILENAME_AUTHOR that is 1. lowercased, 2. spaces replaced with '_' and 3. all punctuation except for dashes and underlines replaced with '-'\n",
        "BOOK_TITLE_AUTHOR_CLEAN = BOOK_FILENAME_AUTHOR_CLEAN.lower().replace(' ', '_').replace('[^a-zA-Z0-9_-]', '-')\n",
        "print(f\"BOOK_TITLE_AUTHOR_CLEAN: {BOOK_TITLE_AUTHOR_CLEAN}\")\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "# prompt: set BOOK_TITLE_AUTHOR_TITLE = BOOK_TITLE_AUTHOR_CLEAN with '_' replaced by ' ' and the first letter of all words Capitalized\n",
        "BOOK_TITLE_AUTHOR_TITLE = BOOK_TITLE_AUTHOR_CLEAN.replace('_', ' ').title()\n",
        "print(f\"BOOK_TITLE_AUTHOR_TITLE: {BOOK_TITLE_AUTHOR_TITLE}\")\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "# prompt: change the leading string 'book_' with 'plot_' in the var BOOK_TITLE_AUTHOR_CLEAN\n",
        "\n",
        "PLOT_FILENAME_AUTHOR_CLEAN = BOOK_FILENAME_AUTHOR_CLEAN.replace('book_', 'plot_')\n",
        "print(f\"PLOT_FILENAME_AUTHOR_CLEAN: {PLOT_FILENAME_AUTHOR_CLEAN}\")\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "# prompt: change the leading string 'book_' with 'data_' in the var BOOK_TITLE_AUTHOR_CLEAN\n",
        "\n",
        "DATA_FILENAME_AUTHOR_CLEAN = BOOK_FILENAME_AUTHOR_CLEAN.replace('book_', 'data_')\n",
        "print(f\"DATA_FILENAME_AUTHOR_CLEAN: {DATA_FILENAME_AUTHOR_CLEAN}\")"
      ],
      "metadata": {
        "id": "i479Jo5eYP5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sw5cff-kNOBL"
      },
      "source": [
        "## Clean Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mWoG6U07jU3"
      },
      "outputs": [],
      "source": [
        "!pip install clean-text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8TQCzw6ZjTA"
      },
      "outputs": [],
      "source": [
        "!pip install unidecode  # clean-text dependency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tnbwxio8NFs8"
      },
      "outputs": [],
      "source": [
        "from cleantext import clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRTdmakW8tO_"
      },
      "outputs": [],
      "source": [
        "novel_clean_str = clean(novel_raw_str,\n",
        "    fix_unicode=True,               # fix various unicode errors\n",
        "    to_ascii=True,                  # transliterate to closest ASCII representation\n",
        "    lower=True,                     # lowercase text\n",
        "    no_line_breaks=False,           # fully strip line breaks as opposed to only normalizing them\n",
        "    no_urls=False,                  # replace all URLs with a special token\n",
        "    no_emails=False,                # replace all email addresses with a special token\n",
        "    no_phone_numbers=False,         # replace all phone numbers with a special token\n",
        "    no_numbers=False,               # replace all numbers with a special token\n",
        "    no_digits=False,                # replace all digits with a special token\n",
        "    no_currency_symbols=False,      # replace all currency symbols with a special token\n",
        "    no_punct=False,                 # remove punctuations\n",
        "    # replace_with_punct=\"\",          # instead of removing punctuations you may replace them\n",
        "    # replace_with_url=\"<URL>\",\n",
        "    # replace_with_email=\"<EMAIL>\",\n",
        "    # replace_with_phone_number=\"<PHONE>\",\n",
        "    # replace_with_number=\"<NUMBER>\",\n",
        "    # replace_with_digit=\"0\",\n",
        "    # replace_with_currency_symbol=\"<CUR>\",\n",
        "    lang=\"en\"                       # set to 'de' for German special handling\n",
        ")\n",
        "\n",
        "# Replace all new lines/returns with single whitespace\n",
        "novel_clean_str = novel_clean_str.replace('\\n\\r', ' ')\n",
        "novel_clean_str = novel_clean_str.replace('\\n', ' ')\n",
        "novel_clean_str = novel_clean_str.replace('\\r', ' ')\n",
        "novel_clean_str = ' '.join(novel_clean_str.split())\n",
        "novel_clean_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIsI8CgED8N7"
      },
      "outputs": [],
      "source": [
        "# Verify\n",
        "\n",
        "verify_novel(novel_clean_str, index_ends=500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5L8ueH7obDFk"
      },
      "source": [
        "## Trim Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhdTUaSqbC5c"
      },
      "outputs": [],
      "source": [
        "# FUTURE: Auto trim Gutenberg HEADER and FOOTER (varies too much)\n",
        "\n",
        "novel_trim_str = novel_clean_str"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDa43-cGOIMa"
      },
      "source": [
        "## Segment Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wz6aVlxzbtEf"
      },
      "outputs": [],
      "source": [
        "# FUTURE: Normally assigned within the 'Trim Header & Footer' Section\n",
        "\n",
        "# Ensure we have trimmed version of novel in novel_trim_str\n",
        "\n",
        "if len(novel_trim_str) > 0:\n",
        "  # Header/Footer already trimmed from body of Novel\n",
        "  pass\n",
        "else:\n",
        "  novel_trim_str = novel_raw_str"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "\n",
        "# FUTURE: Display Segmenting Progress Bar\n",
        "\n",
        "from IPython.display import clear_output\n",
        "import pysbd\n",
        "import time  # Only for simulating delay in testing\n",
        "\n",
        "# Set up the segmenter\n",
        "seg = pysbd.Segmenter(language=\"en\", clean=False)\n",
        "\n",
        "# Segment novel with manual progress feedback\n",
        "novel_segments_ls = []\n",
        "total_segments = len(novel_trim_str.split('.'))  # Estimate segment count based on periods\n",
        "update_every_n = 100  # Update output every 100 segments\n",
        "\n",
        "# Start segmentation and manual progress display\n",
        "for idx, segment in enumerate(seg.segment(novel_trim_str), start=1):\n",
        "    novel_segments_ls.append(segment)\n",
        "\n",
        "    # Periodically update the output\n",
        "    if idx % update_every_n == 0 or idx == total_segments:\n",
        "        clear_output(wait=True)\n",
        "        print(f\"Segmenting Novel: {idx}/{total_segments} segments processed\")\n",
        "\n",
        "print(\"Segmentation complete.\")\n",
        "''';"
      ],
      "metadata": {
        "id": "DZXOKiTKOHWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CONFIGURE: Select segmentation method\n",
        "\n",
        "SEGMENTATION_METHOD_LS = ['nltk','spacy','pysbd','wtpsplit']\n",
        "SEGMENTATION_METHOD = SEGMENTATION_METHOD_LS[0]\n",
        "\n",
        "print(f\"Segmentation Method Selected: {SEGMENTATION_METHOD}\")"
      ],
      "metadata": {
        "id": "cocUWMsKu2F_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if SEGMENTATION_METHOD == 'nltk':\n",
        "  # OPTION (A) Least precise sentence segmentation using NLTK (fast)\n",
        "\n",
        "  print(f\"SEGMENTATION_METHOD: {SEGMENTATION_METHOD}\")\n",
        "  def segment_sentences_nltk(text):\n",
        "      sentences = sent_tokenize(text)\n",
        "      return sentences\n",
        "\n",
        "  novel_segments_ls = segment_sentences_nltk(novel_trim_str)\n",
        "  # print(novel_segments_ls[:5])  # Preview the first few sentences\n",
        "  # print(len(novel_segments_ls))\n",
        "\n",
        "elif SEGMENTATION_METHOD == 'spacy':\n",
        "  # OPTION (B) Mid-precision sentence segmentation using SpaCy (slow for large text)\n",
        "\n",
        "  print(f\"SEGMENTATION_METHOD: {SEGMENTATION_METHOD}\")\n",
        "\n",
        "  import spacy\n",
        "\n",
        "  # Load the small English model in spaCy\n",
        "  nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"ner\", \"parser\"])  # Disable unnecessary components for speed\n",
        "\n",
        "  # Enable the sentence segmentation component only\n",
        "  nlp.enable_pipe(\"senter\")  # spaCy's rule-based sentence segmentation\n",
        "\n",
        "  # Function to segment a large text\n",
        "  def segment_sentences(text):\n",
        "      # Process the text with spaCy\n",
        "      doc = nlp(text)\n",
        "      # Extract each sentence as a string\n",
        "      sentences = [sent.text for sent in doc.sents]\n",
        "      return sentences\n",
        "\n",
        "  # Example usage\n",
        "  novel_trim_str = \"\"\"Your large text goes here...\"\"\"  # Replace with actual text\n",
        "  sentences = segment_sentences(novel_trim_str)\n",
        "\n",
        "  # Check the number of sentences and preview a few\n",
        "  print(f\"Total sentences: {len(sentences)}\")\n",
        "  print(sentences[:5])  # Preview first 5 sentences\n",
        "\n",
        "elif SEGMENTATION_METHOD == 'pysbd':\n",
        "\n",
        "  # OPTION (C) Most precise sentence segmentation using pysbd (very slow for large text, may crash)\n",
        "  print(f\"SEGMENTATION_METHOD: {SEGMENTATION_METHOD}\")\n",
        "\n",
        "  import pysbd\n",
        "\n",
        "  seg = pysbd.Segmenter(language=\"en\", clean=False)\n",
        "  novel_segments_ls = [segment for segment in seg.segment(novel_trim_str)]\n",
        "\n",
        "elif SEGMENTATION_METHOD == 'wtpsplit':\n",
        "\n",
        "  # OPTION (D) Most upto date? (untested)\n",
        "  # https://github.com/segment-any-text/wtpsplit\n",
        "\n",
        "  print(f\"SEGMENTATION_METHOD: {SEGMENTATION_METHOD}\")\n",
        "\n",
        "  from wtpsplit import SaT\n",
        "\n",
        "  sat_sm_adapted = SaT(\"sat-3l-sm\", style_or_domain=\"ud\", language=\"en\")\n",
        "  sat_sm_adapted.half().to(\"cuda\") # optional, see above\n",
        "\n",
        "  novel_segments_ls = [segment for segment in sat_sm_adapted(novel_trim_str)]\n",
        "\n",
        "else:\n",
        "\n",
        "  print(f\"ERROR: Invalid SEGMENTATION_METHOD: {SEGMENTATION_METHOD}\")\n"
      ],
      "metadata": {
        "id": "Gg5y47Q8UXzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19xTLVlAPjlm"
      },
      "outputs": [],
      "source": [
        "# Trim any leading/trailing whitespace on all Sentences\n",
        "\n",
        "novel_clean_ls = [x.strip() for x in novel_segments_ls]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First five lines\n",
        "novel_clean_ls[:5]\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "# Last five line\n",
        "novel_clean_ls[-5:]"
      ],
      "metadata": {
        "id": "4DxQglzSb0En"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62DhkoOkEauJ"
      },
      "outputs": [],
      "source": [
        "# Verify\n",
        "\n",
        "verify_novel(novel_clean_ls, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWECS98kFM1d"
      },
      "outputs": [],
      "source": [
        "# Save to file and download copy\n",
        "\n",
        "save_text2txt_and_download(novel_clean_ls, BOOK_TITLE_AUTHOR_CLEAN, '_segments.txt')\n",
        "time.sleep(3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Delay 3s to allow time for download if [Runtime]->[Run all]\n",
        "\n",
        "time.sleep(3)"
      ],
      "metadata": {
        "id": "ZdaOSyvIv_oW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5Qbh6uHJ_fh"
      },
      "outputs": [],
      "source": [
        "# Populate novel sentiment_df with sentence number and clean segmented strings\n",
        "\n",
        "sentence_no_ls = list(range(len(novel_clean_ls)))\n",
        "sentence_no_ls[-1]\n",
        "\n",
        "sentiment_df = pd.DataFrame({'line_no':sentence_no_ls, 'line':novel_clean_ls})\n",
        "sentiment_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2thPN2r_QJSz"
      },
      "source": [
        "# Compute Sentiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJUoLqF_QWbw"
      },
      "source": [
        "## STEP (1): Symbolic: Lexicons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9C-x51-LaES"
      },
      "source": [
        "### VADER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qma7eu0gQnKy"
      },
      "outputs": [],
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "sid_obj = SentimentIntensityAnalyzer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lng_pNpVQnHu"
      },
      "outputs": [],
      "source": [
        "sentiment_vader_ls = [sid_obj.polarity_scores(asentence)['compound'] for asentence in novel_clean_ls]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckLEiP79jwRX"
      },
      "outputs": [],
      "source": [
        "# Create new VADER DataFrame to save results\n",
        "\n",
        "vader_df = sentiment_df[['line_no', 'line']].copy(deep=True)\n",
        "vader_df['vader'] = pd.Series(sentiment_vader_ls)\n",
        "vader_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot"
      ],
      "metadata": {
        "id": "lbnVjNAe01-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if GLOBAL_WIN_FLAG == True:\n",
        "  # Use Global Window Size as Percentage\n",
        "  win_per = GLOBAL_WIN\n",
        "else:\n",
        "  # Set Custom Window Size as Percentage\n",
        "  win_per = 0.10\n",
        "\n",
        "win_size = int(win_per * vader_df.shape[0])\n",
        "\n",
        "# Create a new figure\n",
        "plt.figure()\n",
        "\n",
        "# Plot the rolling mean of the 'sentiment' column\n",
        "_ = vader_df['vader'].rolling(win_size, center=True).mean().plot(grid=True)\n",
        "\n",
        "# Ensure the plot is rendered and saved correctly\n",
        "plt.title(f\"{BOOK_TITLE_AUTHOR_TITLE}\\nVADER Sentiment Analysis Rolling Mean (win={win_per})\")\n",
        "plt.xlabel(\"Index\")\n",
        "plt.ylabel(\"Sentiment Score\")\n",
        "\n",
        "# Save the plot to a PNG file\n",
        "time.sleep(3)\n",
        "save_plot2png('_sentiment.png', nodate=True, base_filename=f'{PLOT_FILENAME_AUTHOR_CLEAN}_vader')\n",
        "time.sleep(3)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Close the plot to free up memory\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "zPEvaNVVMVq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **[STOP to GRANT PERMISSION]** Pop-up dialog box asks permission to download files"
      ],
      "metadata": {
        "id": "_io4gVHkp2F0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hf9ZTDTUxU_"
      },
      "outputs": [],
      "source": [
        "# Call the function to save the DataFrame to CSV\n",
        "\n",
        "# Delay 3s to allow time for download if [Runtime]->[Run all]\n",
        "time.sleep(3)\n",
        "save_df2csv_and_download(vader_df, '_vader_sentiment.csv', nodate=True, base_filename=DATA_FILENAME_AUTHOR_CLEAN)\n",
        "time.sleep(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_DEOrrfnQKT"
      },
      "source": [
        "### TextBlob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tMg_wp7nQAW"
      },
      "outputs": [],
      "source": [
        "from textblob import TextBlob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8PStLF-nkug"
      },
      "outputs": [],
      "source": [
        "testimonial = TextBlob(\"Textblob is amazingly simple to use. What great fun!\")\n",
        "print(testimonial.sentiment.polarity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjaHzz6g_9bL"
      },
      "outputs": [],
      "source": [
        "sentiment_textblob_ls = [TextBlob(asentence).sentiment.polarity for asentence in novel_clean_ls]\n",
        "# sentiment_df['textblob'] = sentiment_df['text_clean'].apply(lambda x : TextBlob(x).sentiment.polarity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQ5Kr4oH_9YM"
      },
      "outputs": [],
      "source": [
        "# Create new TextBlob DataFrame to save results\n",
        "\n",
        "textblob_df = sentiment_df[['line_no', 'line']].copy(deep=True)\n",
        "textblob_df['textblob'] = pd.Series(sentiment_textblob_ls)\n",
        "textblob_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot"
      ],
      "metadata": {
        "id": "imD89T8s0xbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if GLOBAL_WIN_FLAG == True:\n",
        "  # Use Global Window Size as Percentage\n",
        "  win_per = GLOBAL_WIN\n",
        "else:\n",
        "  # Set Custom Window Size as Percentage\n",
        "  win_per = 0.10\n",
        "\n",
        "win_size = int(win_per * vader_df.shape[0])\n",
        "\n",
        "\n",
        "# Create a new figure\n",
        "plt.figure()\n",
        "\n",
        "# Plot the rolling mean of the 'sentiment' column\n",
        "_ = textblob_df['textblob'].rolling(win_size, center=True).mean().plot(grid=True)\n",
        "\n",
        "# Ensure the plot is rendered and saved correctly\n",
        "plt.title(f\"{BOOK_TITLE_AUTHOR_TITLE}\\nTextBlob Sentiment Analysis Rolling Mean (win={win_per})\")\n",
        "plt.xlabel(\"Index\")\n",
        "plt.ylabel(\"Sentiment Score\")\n",
        "\n",
        "# Save the plot to a PNG file with the updated function\n",
        "time.sleep(3)\n",
        "save_plot2png('_sentiment.png', nodate=True, base_filename=f'{PLOT_FILENAME_AUTHOR_CLEAN}_textblob')\n",
        "time.sleep(3)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Close the plot to free up memory\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "Wx2pVRHT8Yyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save TextBlob Model Sentiment Time Series to CSV with the updated function\n",
        "\n",
        "# Delay 3s to allow time for download if [Runtime]->[Run all]\n",
        "\n",
        "time.sleep(3)\n",
        "save_df2csv_and_download(textblob_df, '_textblob_sentiment.csv', nodate=True, base_filename=DATA_FILENAME_AUTHOR_CLEAN)\n",
        "time.sleep(3)"
      ],
      "metadata": {
        "id": "nsotSUrUNdpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3YQL4FKQdBi"
      },
      "source": [
        "## STEP (2): Connectionist: Transformers\n",
        "\n",
        "**WARNING:** This takes a LONG TIME to run to completion (~45mins).\n",
        "\n",
        "Accelerate Large Models:\n",
        "\n",
        "* https://ponder.io/faster-hugging-face-with-modin/ ***\n",
        "\n",
        "* https://huggingface.co/blog/accelerate-large-models\n",
        "\n",
        "* (Moden) https://github.com/modin-project/modin\n",
        "* (Moden+HF) https://github.com/ponder-org/ponder-blog/blob/main/Modin%20%2B%20Hugging%20Face%20Tutorial.ipynb\n",
        "\n",
        "* https://heartbeat.comet.ml/optimizing-a-huggingface-transformer-model-for-toxic-speech-detection-6d59e66f615a"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer Setup"
      ],
      "metadata": {
        "id": "krn6_flgcNX2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8yQt0LzpJTA"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5Uj6TzEFFoZ"
      },
      "outputs": [],
      "source": [
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLbC-Gx9pEcc"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelWithLMHead  # T5Base 50k\n",
        "from transformers import AutoModelForSequenceClassification, Trainer\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoModelWithLMHead\n",
        "\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "import sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LL27sTjlui1S"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GsluNzHOnSV"
      },
      "outputs": [],
      "source": [
        "# Test Dataset: List of TestSentiment Strings\n",
        "\n",
        "test_lines_ls = [\n",
        "    \"I love you.\",\n",
        "    \"You hate me.\",\n",
        "    \"I'm not sure if I hate you, but I certainly don't care for your attitude young man!\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JizomLuFsOZ_"
      },
      "outputs": [],
      "source": [
        "# Create class for data preparation\n",
        "\n",
        "class SimpleDataset:\n",
        "    def __init__(self, tokenized_texts):\n",
        "        self.tokenized_texts = tokenized_texts\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokenized_texts[\"input_ids\"])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {k: v[idx] for k, v in self.tokenized_texts.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrsQK7ezzDfU"
      },
      "source": [
        "### HF: DistilBERT Finetuned (sst2en)\n",
        "\n",
        "distilbert-base-uncased-finetuned-sst-2-english\n",
        "\n",
        "* https://huggingface.co/docs/transformers/task_summary"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[STOP to GRANT PERMISSION]** to HF_TOKEN\n",
        "\n",
        "In \"Secrets\" configure \"Notebook Access\" for HF_TOKEN or else a pop-up dialog box will STOP here and manually request permissions"
      ],
      "metadata": {
        "id": "1s7xcMWLRt_S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMOGq-X9WIbv"
      },
      "outputs": [],
      "source": [
        "# Load tokenizer and model, create trainer\n",
        "\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "# trainer = Trainer(model=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWGnuCbps9WD"
      },
      "outputs": [],
      "source": [
        "# Create training arguments to disable wandb monitoring\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    report_to=[],  # This disables wandb logging\n",
        ")\n",
        "\n",
        "# Initialize trainer with the arguments\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgIseJpWWbYq"
      },
      "outputs": [],
      "source": [
        "# Tokenize texts and create prediction data set\n",
        "\n",
        "line_ls = sentiment_df['line'].to_list()\n",
        "\n",
        "tokenized_texts = tokenizer(line_ls,truncation=True,padding=True)\n",
        "pred_dataset = SimpleDataset(tokenized_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivgHsGylejO_"
      },
      "outputs": [],
      "source": [
        "# import wandb\n",
        "\n",
        "# wandb.init() # wandb.log()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylqEUWutWbYq"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# NOTE: Must click on output LINK, get wandb API_KEY, and paste it in text input box\n",
        "\n",
        "# NOTE: 0m40s 02:49EST on 28Oct2022 Colab Pro (The Idiot)\n",
        "\n",
        "\n",
        "# Run predictions\n",
        "predictions = trainer.predict(pred_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyAE2QZVWbYq"
      },
      "outputs": [],
      "source": [
        "type(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4vzPeFoWbYq"
      },
      "outputs": [],
      "source": [
        "# Transform predictions to labels\n",
        "sentiment_ls = predictions.predictions.argmax(-1)\n",
        "labels_ls = pd.Series(sentiment_ls).map(model.config.id2label)\n",
        "scores_ls = (np.exp(predictions[0])/np.exp(predictions[0]).sum(-1,keepdims=True)).max(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKdY2KTVWbYq"
      },
      "outputs": [],
      "source": [
        "line_no_ls = list(range(len(sentiment_ls)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1W968Sz8WbYr"
      },
      "outputs": [],
      "source": [
        "# Create DataFrame with texts, predictions, labels, and scores\n",
        "distilbert_df = pd.DataFrame(list(zip(line_no_ls, line_ls,sentiment_ls,labels_ls,scores_ls)), columns=['line_no','line','distilbert','label','score'])\n",
        "distilbert_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMLpdjoAWbYr"
      },
      "outputs": [],
      "source": [
        "distilbert_df['label'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot"
      ],
      "metadata": {
        "id": "ntK4Hlzq0_6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot VADER\n",
        "\n",
        "if GLOBAL_WIN_FLAG == True:\n",
        "  # Use Global Window Size as Percentage\n",
        "  win_per = GLOBAL_WIN\n",
        "else:\n",
        "  # Set Custom Window Size as Percentage\n",
        "  win_per = 0.10\n",
        "\n",
        "win_size = int(win_per * vader_df.shape[0])\n",
        "\n",
        "# Create a new figure\n",
        "plt.figure()\n",
        "\n",
        "# Plot the rolling mean of the 'distilbert' sentiment column\n",
        "_ = distilbert_df['distilbert'].rolling(win_size, center=True).mean().plot(grid=True)\n",
        "\n",
        "# Ensure the plot is rendered and saved correctly\n",
        "plt.title(f\"{BOOK_TITLE_AUTHOR_TITLE}\\nDistilBERT Sentiment Analysis Rolling Mean (win={win_per})\")\n",
        "plt.xlabel(\"Index\")\n",
        "plt.ylabel(\"Sentiment Score\")\n",
        "\n",
        "# Save the plot to a PNG file with the updated function\n",
        "time.sleep(3)\n",
        "save_plot2png('_sentiment.png', nodate=True, base_filename=f'{PLOT_FILENAME_AUTHOR_CLEAN}_distilbert')\n",
        "time.sleep(3)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Close the plot to free up memory\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "7OTXkxyy8pDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Model Sentiment Time Series to CSV with the updated function\n",
        "\n",
        "# Delay 3s to allow time for download if [Runtime]->[Run all]\n",
        "time.sleep(3)\n",
        "save_df2csv_and_download(distilbert_df, '_distilbert_sentiment.csv', nodate=True, base_filename=DATA_FILENAME_AUTHOR_CLEAN)\n",
        "time.sleep(3)"
      ],
      "metadata": {
        "id": "LBgTXnGQOQhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMk2CQeViO40"
      },
      "source": [
        "### HF: MultiBERT NLPTown\n",
        "\n",
        "nlptown/bert-base-multilingual-uncased-sentiment\n",
        "\n",
        "* https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment?text=I+like+you.+I+love+you"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bPn93D-9un1"
      },
      "outputs": [],
      "source": [
        "# Load tokenizer and model, create trainer\n",
        "\n",
        "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "# trainer = Trainer(model=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDJG5iEu-CM_"
      },
      "outputs": [],
      "source": [
        "# Tokenize texts and create prediction data set\n",
        "\n",
        "# Tokenize texts and create prediction data set\n",
        "\n",
        "line_ls = sentiment_df['line'].to_list()\n",
        "\n",
        "tokenized_texts = tokenizer(line_ls,truncation=True,padding=True)\n",
        "pred_dataset = SimpleDataset(tokenized_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1guvwFsIvVuJ"
      },
      "outputs": [],
      "source": [
        "# Create training arguments with wandb disabled\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    report_to=[],  # This disables wandb logging\n",
        ")\n",
        "\n",
        "# Initialize trainer with the arguments\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_6bxme_vVuJ"
      },
      "outputs": [],
      "source": [
        "# Tokenize texts and create prediction data set\n",
        "tokenized_texts = tokenizer(line_ls,truncation=True,padding=True)\n",
        "pred_dataset = SimpleDataset(tokenized_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bE63EELQ-CM_"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# NOTE:  4m00s 23:57EST on 27Oct2022 Colab Pro (The Idiot)\n",
        "#        1m28s 01:24EST on 28Oct2022 Colab Pro (The Idiot)\n",
        "#        1m27s 02:42EST on 28Oct2022 Colab Pro (The Idiot)\n",
        "\n",
        "# Run predictions\n",
        "predictions = trainer.predict(pred_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ft9zucAJ-CM_"
      },
      "outputs": [],
      "source": [
        "# Transform predictions to labels\n",
        "sentiment_ls = predictions.predictions.argmax(-1)\n",
        "labels_ls = pd.Series(sentiment_ls).map(model.config.id2label)\n",
        "scores_ls = (np.exp(predictions[0])/np.exp(predictions[0]).sum(-1,keepdims=True)).max(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdA3_IeGUPff"
      },
      "outputs": [],
      "source": [
        "line_no_ls = list(range(len(sentiment_ls)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHKwQi2k-CM_"
      },
      "outputs": [],
      "source": [
        "# Create DataFrame with texts, predictions, labels, and scores\n",
        "nlptown_df = pd.DataFrame(list(zip(line_no_ls,line_ls,sentiment_ls,labels_ls,scores_ls)), columns=['line_no','line','nlptown','label','score'])\n",
        "nlptown_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xzy-u9cJUnfk"
      },
      "outputs": [],
      "source": [
        "nlptown_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBvG4K7i-CNA"
      },
      "outputs": [],
      "source": [
        "nlptown_df['label'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot"
      ],
      "metadata": {
        "id": "WCMPSj_u1I-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if GLOBAL_WIN_FLAG == True:\n",
        "  # Use Global Window Size as Percentage\n",
        "  win_per = GLOBAL_WIN\n",
        "else:\n",
        "  # Set Custom Window Size as Percentage\n",
        "  win_per = 0.10\n",
        "\n",
        "win_size = int(win_per * vader_df.shape[0])\n",
        "\n",
        "# Create a new figure\n",
        "plt.figure()\n",
        "\n",
        "# Plot the rolling mean of the 'nlptown' sentiment column\n",
        "_ = nlptown_df['nlptown'].rolling(win_size, center=True).mean().plot(grid=True)\n",
        "\n",
        "# Ensure the plot is rendered and saved correctly\n",
        "plt.title(f\"{BOOK_TITLE_AUTHOR_TITLE}\\nNLPTown Sentiment Analysis Rolling Mean (win={win_per})\")\n",
        "plt.xlabel(\"Index\")\n",
        "plt.ylabel(\"Sentiment Score\")\n",
        "\n",
        "# Save the plot to a PNG file with the updated function\n",
        "\n",
        "# Delay 3s to allow time for download if [Runtime]->[Run all]\n",
        "time.sleep(3)\n",
        "save_plot2png('_sentiment.png', nodate=True, base_filename=f'{PLOT_FILENAME_AUTHOR_CLEAN}_nlptown')\n",
        "time.sleep(3)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Close the plot to free up memory\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "Ig7TLujAOcls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Model Sentiment Time Series to CSV with the updated function\n",
        "\n",
        "# Delay 3s to allow time for download if [Runtime]->[Run all]\n",
        "time.sleep(3)\n",
        "save_df2csv_and_download(nlptown_df, '_nlptown_sentiment.csv', nodate=True, base_filename=DATA_FILENAME_AUTHOR_CLEAN)\n",
        "time.sleep(3)"
      ],
      "metadata": {
        "id": "AX4rx_0sOl5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVFTFSU9pVPp"
      },
      "source": [
        "### HF: RoBERTa Finetuned (en)\n",
        "\n",
        "siebert/sentiment-roberta-large-english\n",
        "\n",
        "* https://colab.research.google.com/github/chrsiebert/sentiment-roberta-large-english/blob/main/sentiment_roberta_prediction_example.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIQKzZKVr-aq"
      },
      "outputs": [],
      "source": [
        "# Load tokenizer and model, create trainer\n",
        "\n",
        "model_name = \"siebert/sentiment-roberta-large-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "# trainer = Trainer(model=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-L7j_8xsrpn"
      },
      "outputs": [],
      "source": [
        "# Create training arguments with wandb disabled\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    report_to=[],  # This disables wandb logging\n",
        ")\n",
        "\n",
        "# Initialize trainer with the arguments\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jtKBTZesz-A"
      },
      "outputs": [],
      "source": [
        "# Tokenize texts and create prediction data set\n",
        "\n",
        "line_ls = sentiment_df['line'].to_list()\n",
        "\n",
        "tokenized_texts = tokenizer(line_ls,truncation=True,padding=True)\n",
        "pred_dataset = SimpleDataset(tokenized_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5mjnob3sMCl"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# NOTE: 4m00s 23:57EST on 27Oct2022 Colab Pro (The Idiot)\n",
        "#       4m18s 02:27EST on 27Oct2022 Colab Pro (The Idiot)\n",
        "\n",
        "# Run predictions\n",
        "predictions = trainer.predict(pred_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daSsWIMGSRhV"
      },
      "outputs": [],
      "source": [
        "type(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3O53RHCsVd7"
      },
      "outputs": [],
      "source": [
        "# Transform predictions to labels\n",
        "sentiment_ls = predictions.predictions.argmax(-1)\n",
        "labels_ls = pd.Series(sentiment_ls).map(model.config.id2label)\n",
        "scores_ls = (np.exp(predictions[0])/np.exp(predictions[0]).sum(-1,keepdims=True)).max(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULY4BunEQp7f"
      },
      "outputs": [],
      "source": [
        "line_no_ls = list(range(len(sentiment_ls)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhIONI7ett0q"
      },
      "outputs": [],
      "source": [
        "# Create DataFrame with texts, predictions, labels, and scores\n",
        "roberta15lg_df = pd.DataFrame(list(zip(line_no_ls, line_ls,sentiment_ls,labels_ls,scores_ls)), columns=['line_no','line','roberta15lg','label','score'])\n",
        "roberta15lg_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JCLRT4dwidn"
      },
      "outputs": [],
      "source": [
        "roberta15lg_df['label'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot"
      ],
      "metadata": {
        "id": "sicFPjiE1TXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if GLOBAL_WIN_FLAG == True:\n",
        "  # Use Global Window Size as Percentage\n",
        "  win_per = GLOBAL_WIN\n",
        "else:\n",
        "  # Set Custom Window Size as Percentage\n",
        "  win_per = 0.10\n",
        "\n",
        "win_size = int(win_per * vader_df.shape[0])\n",
        "\n",
        "# Create a new figure\n",
        "plt.figure()\n",
        "\n",
        "# Plot the rolling mean of the 'roberta15lg' sentiment column\n",
        "_ = roberta15lg_df['roberta15lg'].rolling(win_size, center=True).mean().plot(grid=True)\n",
        "\n",
        "# Ensure the plot is rendered and saved correctly\n",
        "plt.title(f\"{BOOK_TITLE_AUTHOR_TITLE}\\nRoBERTa Sentiment Analysis Rolling Mean (win={win_per})\")\n",
        "plt.xlabel(\"Index\")\n",
        "plt.ylabel(\"Sentiment Score\")\n",
        "\n",
        "# Save the plot to a PNG file with the updated function\n",
        "\n",
        "# Delay 3s to allow time for download if [Runtime]->[Run all]\n",
        "time.sleep(3)\n",
        "save_plot2png('_sentiment.png', nodate=True, base_filename=f'{PLOT_FILENAME_AUTHOR_CLEAN}_roberta15lg')\n",
        "time.sleep(3)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Close the plot to free up memory\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "U6EWD50jOzmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Model Sentiment Time Series to CSV with the updated function\n",
        "\n",
        "# Delay 3s to allow time for download if [Runtime]->[Run all]\n",
        "time.sleep(3)\n",
        "save_df2csv_and_download(roberta15lg_df, '_roberta15lg_sentiment.csv', nodate=True, base_filename=DATA_FILENAME_AUTHOR_CLEAN)\n",
        "time.sleep(3)"
      ],
      "metadata": {
        "id": "iBkRvYBJPPaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLMS4igchaNH"
      },
      "source": [
        "# Combined Plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frnJrT2F-DT5"
      },
      "source": [
        "## Dialate SentimentR, Combine, and Plot Together\n",
        "\n",
        "* VADER\n",
        "* TextBlob\n",
        "* SyuzhetR (4)\n",
        "* DistilBERT\n",
        "* NLPTown\n",
        "* RoBERTa15lg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ywwy_xDQJ4b"
      },
      "outputs": [],
      "source": [
        "def make_csv_list(filename_root):\n",
        "    # List of possible model names\n",
        "    model_ls = ['vader', 'textblob', 'syuzhetr', 'sentimentr', 'distilbert', 'nlptown', 'roberta15lg']\n",
        "\n",
        "    # Remove the file extension and extract the title_author part\n",
        "    filename_without_ext = os.path.splitext(filename_root)[0]\n",
        "    parts = filename_without_ext.split('_')\n",
        "    title_author = '_'.join(parts[1:3])  # Get both parts of title_author (e.g., \"dracula_bramstoker\")\n",
        "\n",
        "    # Initialize an empty list to hold the found CSV files\n",
        "    csv_list = []\n",
        "\n",
        "    # Debugging information\n",
        "    print(f\"Looking for files with title_author: {title_author}\")\n",
        "\n",
        "    # Iterate through each model name and construct the expected filename\n",
        "    for model_name in model_ls:\n",
        "        # Construct the filename based on the new pattern\n",
        "        csv_filename = f'data_{title_author}_{model_name}_sentiment.csv'\n",
        "        print(f\"Checking for file: {csv_filename}\")\n",
        "\n",
        "        # Check if the file exists in the current directory\n",
        "        if os.path.isfile(csv_filename):\n",
        "            csv_list.append(csv_filename)\n",
        "        else:\n",
        "            print(f\"File not found: {csv_filename}\")\n",
        "\n",
        "    return csv_list\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sentiment_all_df."
      ],
      "metadata": {
        "id": "ti-FIKP3VX5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSy5_q04bS4t"
      },
      "outputs": [],
      "source": [
        "# NOTE: Your uploaded text files should have 3 parts separated by two '_' underscores\n",
        "# \"book\" + \"_\" + \"title-words-separated-by-hyphens\" + \"_\" + \"author\" (where author is one word with only alphanumeric chars, no hypens or other punctuations)\n",
        "\n",
        "# BOOK_FILENAME_AUTHOR_CLEAN = 'book_jane-eyre_cbronte'\n",
        "# BOOK_FILENAME_AUTHOR_CLEAN = novel_name_str\n",
        "BOOK_FILENAME_AUTHOR_CLEAN"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: write a python function add_data_prefix_to_csv(cur_dir=\".\") that add the prefix \"data_\" to all the *.csv files in the current directory cur_dir\n",
        "\n",
        "def add_data_prefix_to_csv(cur_dir=\".\"):\n",
        "    for filename in os.listdir(cur_dir):\n",
        "        if filename.endswith(\".csv\"):\n",
        "            new_filename = \"data_\" + filename\n",
        "            os.rename(os.path.join(cur_dir, filename), os.path.join(cur_dir, new_filename))\n",
        "\n",
        "# add_data_prefix_to_csv()"
      ],
      "metadata": {
        "id": "cZw_jd00yfpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: write a funtion insert_string_after_data_prefix(cur_dir='.') that inserts the string 'kamala_' after every *.csv file that starts with data_harris\n",
        "\n",
        "def insert_string_after_data_prefix(cur_dir='.'):\n",
        "    for filename in os.listdir(cur_dir):\n",
        "        if filename.startswith(\"data_harris\") and filename.endswith(\".csv\"):\n",
        "            new_filename = filename.replace(\"data_harris\", \"data_harris_kamala_\", 1)\n",
        "            os.rename(os.path.join(cur_dir, filename), os.path.join(cur_dir, new_filename))\n",
        "\n",
        "# insert_string_after_data_prefix()"
      ],
      "metadata": {
        "id": "UnT34AZMy2ZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls *.csv"
      ],
      "metadata": {
        "id": "praBewiF0vdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YrhJIFajXwS1"
      },
      "outputs": [],
      "source": [
        "# Example usage\n",
        "csv_list = make_csv_list(BOOK_FILENAME_AUTHOR_CLEAN)\n",
        "print(csv_list)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BOOK_FILENAME_AUTHOR_CLEAN"
      ],
      "metadata": {
        "id": "q0uNZWHFzOiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3u9x-VrY-DKz"
      },
      "outputs": [],
      "source": [
        "models_sa_ls = make_csv_list(BOOK_FILENAME_AUTHOR_CLEAN)\n",
        "for amodel in models_sa_ls:\n",
        "  print(amodel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ex7ICmHvTbRp"
      },
      "outputs": [],
      "source": [
        "# models_sa_ls.remove('book_the-great-gatsby_fscottfitzgerald_sentimentr.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpq4vACRS-cL"
      },
      "outputs": [],
      "source": [
        "for amodel in models_sa_ls:\n",
        "  print(f\"\\n\\n{amodel}\")\n",
        "  temp_df = pd.read_csv(amodel)\n",
        "  print(temp_df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(models_sa_ls)"
      ],
      "metadata": {
        "id": "h-uOr2VwDMtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models_sa_ls"
      ],
      "metadata": {
        "id": "HtYIAEBFxokF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_cs1LU-0X2gL"
      },
      "outputs": [],
      "source": [
        "sentiment_all_df = pd.read_csv(models_sa_ls[0])  # Start with vader\n",
        "\n",
        "for i, amodel in enumerate(models_sa_ls[1:]):\n",
        "    model_name_str = amodel.split('.')[0].split('_')[-1].lower().strip()\n",
        "    print(f'Model #{i}: {model_name_str}')\n",
        "\n",
        "    one_model_df = pd.read_csv(amodel)\n",
        "    print(f' Cols: {one_model_df.columns.to_list()}')\n",
        "\n",
        "    # Special handling for sentimentr which has different structure\n",
        "    if 'sentimentr' in model_name_str:\n",
        "        # Add line_no as index+1 since sentimentr is missing it\n",
        "        one_model_df['line_no'] = range(1, len(one_model_df) + 1)\n",
        "\n",
        "    # Remove unnecessary columns\n",
        "    cols_to_drop = ['line', 'label', 'score']\n",
        "    for col in cols_to_drop:\n",
        "        if col in one_model_df.columns:\n",
        "            one_model_df.drop(columns=[col], inplace=True)\n",
        "\n",
        "    # Rename sentiment column if it exists\n",
        "    if 'sentiment' in one_model_df.columns:\n",
        "        one_model_df.rename(columns={'sentiment': model_name_str}, inplace=True)\n",
        "\n",
        "    # Merge on line_no\n",
        "    sentiment_all_df = sentiment_all_df.merge(one_model_df, on='line_no')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_all_df.info()"
      ],
      "metadata": {
        "id": "jajZ2MjK5U4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: for sentiment_all_df columns rename sentiment_x and sentiment_y to vader and textblob\n",
        "\n",
        "sentiment_all_df.rename(columns={'sentiment_x': 'vader', 'sentiment_y': 'textblob'}, inplace=True)\n",
        "sentiment_all_df.info()"
      ],
      "metadata": {
        "id": "NAK_srB95SLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot"
      ],
      "metadata": {
        "id": "5SY6JBJA2BBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if GLOBAL_WIN_FLAG == True:\n",
        "  # Use Global Window Size as Percentage\n",
        "  win_per = GLOBAL_WIN\n",
        "else:\n",
        "  # Set Custom Window Size as Percentage\n",
        "  win_per = 0.10\n",
        "\n",
        "win_size = int(win_per * vader_df.shape[0])\n",
        "\n",
        "\n",
        "# Assuming sentiment_all_df is your DataFrame containing sentiment scores for different models\n",
        "# and it has columns 'vader', 'textblob', 'distilbert', 'nlptown', 'roberta15lg'\n",
        "\n",
        "# Z-score normalize the sentiment scores\n",
        "for col in ['vader', 'textblob', 'distilbert', 'nlptown', 'roberta15lg']:\n",
        "    sentiment_all_df[col] = zscore(sentiment_all_df[col])\n",
        "\n",
        "for col in ['vader', 'textblob', 'distilbert', 'nlptown', 'roberta15lg']:\n",
        "    sentiment_all_df[f'{col}_sma'] = sentiment_all_df[col].rolling(win_size, center=True).mean()\n",
        "\n",
        "# Plot the SMA of each model on the same plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "for col in ['vader_sma', 'textblob_sma', 'distilbert_sma', 'nlptown_sma', 'roberta15lg_sma']:\n",
        "    plt.plot(sentiment_all_df[col], label=col.replace('_sma', ''))\n",
        "\n",
        "plt.title(f'{BOOK_TITLE_AUTHOR_CLEAN}\\nSMA of Z-score Normalized Sentiment Scores (win={win_per})')\n",
        "plt.xlabel('Index')\n",
        "plt.ylabel('Sentiment Score')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Save the plot to a PNG file\n",
        "\n",
        "# Delay 3s to allow time for download if [Runtime]->[Run all]\n",
        "time.sleep(3)\n",
        "save_plot2png('_sentiment.png', nodate=True, base_filename=f'{PLOT_FILENAME_AUTHOR_CLEAN}_sma_all')\n",
        "time.sleep(3)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Close the plot to free up memory\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "ef8egoTBggWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the function to plot the correlation heatmap\n",
        "def plot_correlation_heatmap(df, columns, title=\"Correlation Matrix of Sentiment Scores\", save_filename=\"\"):\n",
        "    \"\"\"\n",
        "    Plot a correlation heatmap for specified columns in a DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    - df (pd.DataFrame): The DataFrame containing sentiment scores.\n",
        "    - columns (list): List of columns to calculate correlations for.\n",
        "    - title (str): Title for the heatmap plot.\n",
        "    - save_filename (str): Optional filename to save the plot.\n",
        "    \"\"\"\n",
        "    correlation_matrix = df[columns].corr()\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(correlation_matrix, annot=True, cmap='RdBu', center=0, fmt='.2f')\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_filename:\n",
        "      time.sleep(3)\n",
        "      save_plot2png('_sentiment.png', nodate=True, base_filename=f'{save_filename}_sma_all')\n",
        "      time.sleep(3)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    # Close the plot to free up memory\n",
        "    plt.close()\n",
        "\n",
        "# Call plot_correlation_heatmap with the z-score normalized columns\n",
        "plot_correlation_heatmap(\n",
        "    sentiment_all_df,\n",
        "    columns=['vader', 'textblob', 'distilbert', 'nlptown', 'roberta15lg'],\n",
        "    title=f\"Correlation Matrix of Z-score Normalized Sentiment Scores (win={win_per})\",\n",
        "    save_filename=f'{PLOT_FILENAME_AUTHOR_CLEAN}_correlation_heatmap'\n",
        ")\n"
      ],
      "metadata": {
        "id": "US8qmxVE_oQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Delay 3s to allow time for download if [Runtime]->[Run all]\n",
        "\n",
        "time.sleep(3)"
      ],
      "metadata": {
        "id": "UhgxgTxBwRQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Model Sentiment Time Series to CSV with the updated function\n",
        "\n",
        "# Delay 3s to allow time for download if [Runtime]->[Run all]\n",
        "time.sleep(3)\n",
        "save_df2csv_and_download(roberta15lg_df, '_sma_all_sentiment.csv', nodate=True, base_filename=DATA_FILENAME_AUTHOR_CLEAN)\n",
        "time.sleep(3)"
      ],
      "metadata": {
        "id": "vnmd1eej2wrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot_all(sentiment_all_df)"
      ],
      "metadata": {
        "id": "pcZL7X88aVcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzQn4Bd0UJd7"
      },
      "source": [
        "# Crux Detection and Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSQOjfWrrGOo"
      },
      "source": [
        "## Smooth Plots and Add Crux Points"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Choose Specific Model SentimentArc or Mean of All"
      ],
      "metadata": {
        "id": "vL3op9XmMKSP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QUHqy0TYnox"
      },
      "outputs": [],
      "source": [
        "def get_sentiment_mean(df):\n",
        "    \"\"\"Same as before\"\"\"\n",
        "    sentiment_cols = [col for col in df.columns\n",
        "                     if col not in ['line', 'line_no']]\n",
        "    return df[sentiment_cols].mean(axis=1).tolist()\n",
        "\n",
        "\n",
        "def detect_peaks_valleys(smoothed_series, peak_algo='find_peaks', peak_algo_params=None):\n",
        "    \"\"\"\n",
        "    Detect peaks and valleys using specified algorithm.\n",
        "\n",
        "    Args:\n",
        "        smoothed_series (list): Smoothed and normalized time series\n",
        "        peak_algo (str): Peak detection algorithm ('find_peaks', 'relmax', 'threshold')\n",
        "        peak_algo_params (dict): Parameters for the chosen algorithm\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary with 'peaks' and 'valleys' indices\n",
        "    \"\"\"\n",
        "    series = np.array(smoothed_series)\n",
        "\n",
        "    # Default parameters for each algorithm\n",
        "    default_params = {\n",
        "        'find_peaks': {\n",
        "            'distance': 20,\n",
        "            'prominence': 0.1\n",
        "        },\n",
        "        'relmax': {\n",
        "            'order': 10\n",
        "        },\n",
        "        'threshold': {\n",
        "            'threshold': 0.5,\n",
        "            'min_dist': 20\n",
        "        }\n",
        "    }\n",
        "\n",
        "    params = peak_algo_params if peak_algo_params is not None else default_params[peak_algo]\n",
        "\n",
        "    if peak_algo == 'find_peaks':\n",
        "        peaks, _ = find_peaks(series,\n",
        "                            distance=params['distance'],\n",
        "                            prominence=params['prominence'])\n",
        "        valleys, _ = find_peaks(-series,\n",
        "                              distance=params['distance'],\n",
        "                              prominence=params['prominence'])\n",
        "\n",
        "    elif peak_algo == 'relmax':\n",
        "        peaks = argrelextrema(series, np.greater,\n",
        "                            order=params['order'])[0]\n",
        "        valleys = argrelextrema(series, np.less,\n",
        "                              order=params['order'])[0]\n",
        "\n",
        "    elif peak_algo == 'threshold':\n",
        "        peaks = []\n",
        "        valleys = []\n",
        "        min_dist = params['min_dist']\n",
        "        threshold = params['threshold']\n",
        "\n",
        "        for i in range(1, len(series)-1):\n",
        "            if peaks and i - peaks[-1] < min_dist:\n",
        "                continue\n",
        "            if valleys and i - valleys[-1] < min_dist:\n",
        "                continue\n",
        "\n",
        "            if (series[i] > series[i-1] + threshold and\n",
        "                series[i] > series[i+1] + threshold):\n",
        "                peaks.append(i)\n",
        "            elif (series[i] < series[i-1] - threshold and\n",
        "                  series[i] < series[i+1] - threshold):\n",
        "                valleys.append(i)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Peak detection algorithm '{peak_algo}' not implemented. \"\n",
        "                        \"Use 'find_peaks', 'relmax', or 'threshold'.\")\n",
        "\n",
        "    return {\n",
        "        'peaks': peaks.tolist() if isinstance(peaks, np.ndarray) else peaks,\n",
        "        'valleys': valleys.tolist() if isinstance(valleys, np.ndarray) else valleys\n",
        "    }\n",
        "\n",
        "def plot_cruxes(book_name, sentiment_mean_ls, sentiment_mean_smooth_ls,\n",
        "                peak_algo='find_peaks', peak_algo_params=None, SIMPLE=False):\n",
        "    \"\"\"\n",
        "    Plot sentiment analysis with marked crux points.\n",
        "\n",
        "    Args:\n",
        "        book_name (str): Name of the book being analyzed\n",
        "        sentiment_mean_ls (list): Original sentiment values\n",
        "        sentiment_mean_smooth_ls (list): Smoothed sentiment values\n",
        "        peak_algo (str): Peak detection algorithm to use\n",
        "        peak_algo_params (dict): Parameters for peak detection\n",
        "        SIMPLE (bool): If True, only plot smoothed series with crux lines\n",
        "    \"\"\"\n",
        "    # Detect peaks and valleys\n",
        "    crux_points_dt = detect_peaks_valleys(sentiment_mean_smooth_ls,\n",
        "                                        peak_algo=peak_algo,\n",
        "                                        peak_algo_params=peak_algo_params)\n",
        "\n",
        "    plt.figure(figsize=(15, 8))\n",
        "\n",
        "    x = range(len(sentiment_mean_ls))\n",
        "\n",
        "    if not SIMPLE:\n",
        "        plt.plot(x, sentiment_mean_ls, 'lightgray', label='Original', alpha=0.5)\n",
        "\n",
        "    plt.plot(x, sentiment_mean_smooth_ls, 'blue', label='Smoothed', linewidth=2)\n",
        "\n",
        "    # Plot peaks with green dashed lines\n",
        "    y_min, y_max = plt.ylim()\n",
        "    text_y = y_max + 0.05 * (y_max - y_min)\n",
        "\n",
        "    for peak in crux_points_dt['peaks']:\n",
        "        plt.axvline(x=peak, color='green', linestyle='--', alpha=0.5)\n",
        "        plt.text(peak, text_y, f'line {peak}',\n",
        "                rotation=90, va='bottom', ha='center')\n",
        "\n",
        "    # Plot valleys with red dashed lines\n",
        "    for valley in crux_points_dt['valleys']:\n",
        "        plt.axvline(x=valley, color='red', linestyle='--', alpha=0.5)\n",
        "        plt.text(valley, text_y, f'line {valley}',\n",
        "                rotation=90, va='bottom', ha='center')\n",
        "\n",
        "    # Customize plot\n",
        "    plt.title(f'Sentiment Analysis: {book_name}\\nPeak Detection: {peak_algo}')\n",
        "    plt.xlabel('Line Number')\n",
        "    plt.ylabel('Sentiment Score')\n",
        "\n",
        "    if SIMPLE:\n",
        "        plt.legend(['Normalized Sentiment',\n",
        "                   'Peak Lines', 'Valley Lines'])\n",
        "    else:\n",
        "        plt.legend()\n",
        "\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Save plot\n",
        "\n",
        "    time.sleep(3)\n",
        "    plt.savefig(f'{book_name}_sentiment_plot_{peak_algo}.png',\n",
        "                dpi=300, bbox_inches='tight')\n",
        "    time.sleep(3)\n",
        "\n",
        "    plt.close()\n",
        "\n",
        "    return crux_points_dt  # Return detected points for further analysis\n",
        "\n",
        "def smooth_ts(time_series, method='sma', smooth_param='0.1'):\n",
        "    \"\"\"Same as before\"\"\"\n",
        "    series = np.array(time_series)\n",
        "\n",
        "    if method == 'sma':\n",
        "        window = max(3, int(float(smooth_param) * len(series)))\n",
        "        if window % 2 == 0:\n",
        "            window += 1\n",
        "        pad_size = window // 2\n",
        "        padded_series = np.pad(series, (pad_size, pad_size), mode='edge')\n",
        "        smoothed = np.convolve(padded_series, np.ones(window)/window, mode='valid')\n",
        "\n",
        "    elif method == 'lowess':\n",
        "        x = np.arange(len(series))\n",
        "        smoothed = lowess(\n",
        "            series,\n",
        "            x,\n",
        "            frac=float(smooth_param),\n",
        "            return_sorted=False\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Smoothing method '{method}' not implemented. Use 'sma' or 'lowess'.\")\n",
        "\n",
        "    smoothed_min = np.min(smoothed)\n",
        "    smoothed_max = np.max(smoothed)\n",
        "    smoothed_normalized = 2 * (smoothed - smoothed_min) / (smoothed_max - smoothed_min) - 1\n",
        "\n",
        "    return smoothed_normalized.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. CONFIGURE: Select which model or mean(all) to plot/extract cruxes\n",
        "\n",
        "# Calculate mean sentiment across all columns\n",
        "\n",
        "MODEL_INDEX = -1 # set to -1 to use mean of all models\n",
        "\n",
        "model_ls = ['vader','textblob','distilbert','nlptown','roberta15lg']\n",
        "if MODEL_INDEX > -1:\n",
        "  model_name = model_ls[MODEL_INDEX]\n",
        "  sentiment_mean_ls = sentiment_all_df[model_name].tolist()\n",
        "else:\n",
        "  model_name = 'mean'\n",
        "  sentiment_mean_ls = get_sentiment_mean(sentiment_all_df)\n"
      ],
      "metadata": {
        "id": "HbVBzWks6bC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_cruxes(model_name, plot_title, sentiment_mean_ls, sentiment_mean_smooth_ls,\n",
        "                smooth_algo, peak_algo='find_peaks', peak_algo_params=None, SIMPLE=False):\n",
        "    \"\"\"\n",
        "    Plot sentiment analysis with marked crux points.\n",
        "\n",
        "    Args:\n",
        "        book_name (str): Name of the book being analyzed\n",
        "        sentiment_mean_ls (list): Original sentiment values\n",
        "        sentiment_mean_smooth_ls (list): Smoothed sentiment values\n",
        "        peak_algo (str): Peak detection algorithm to use\n",
        "        peak_algo_params (dict): Parameters for peak detection\n",
        "        SIMPLE (bool): If True, only plot smoothed series with crux lines\n",
        "    \"\"\"\n",
        "    # Detect peaks and valleys\n",
        "    crux_points_dt = detect_peaks_valleys(sentiment_mean_smooth_ls,\n",
        "                                        peak_algo=peak_algo,\n",
        "                                        peak_algo_params=peak_algo_params)\n",
        "\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    x = range(len(sentiment_mean_ls))\n",
        "\n",
        "    if not SIMPLE:\n",
        "        plt.plot(x, sentiment_mean_ls, 'lightgray', label='Original', alpha=0.5)\n",
        "\n",
        "    plt.plot(x, sentiment_mean_smooth_ls, 'blue', label='Smoothed', linewidth=2)\n",
        "\n",
        "    # Plot peaks and valleys\n",
        "    y_min, y_max = plt.ylim()\n",
        "    text_y = y_min + 0.9 * (y_max - y_min)\n",
        "\n",
        "    for peak in crux_points_dt['peaks']:\n",
        "        plt.axvline(x=peak, color='green', linestyle='--', alpha=0.5)\n",
        "        plt.text(peak, text_y, f'line {peak}', rotation=90, va='center', ha='center')\n",
        "\n",
        "    for valley in crux_points_dt['valleys']:\n",
        "        plt.axvline(x=valley, color='red', linestyle='--', alpha=0.5)\n",
        "        plt.text(valley, text_y, f'line {valley}', rotation=90, va='center', ha='center')\n",
        "\n",
        "    # Customize plot\n",
        "    # plt.title(f'{BOOK_TITLE_AUTHOR_CLEAN}\\nPeak Detection: {peak_algo}')\n",
        "    plt.title(plot_title)\n",
        "    plt.xlabel('Line Number')\n",
        "    plt.ylabel('Sentiment Score')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save plot before showing it\n",
        "    plot_filename = f'{PLOT_FILENAME_AUTHOR_CLEAN}_{model_name}_{smooth_algo}_cruxes_{peak_algo}.png'\n",
        "\n",
        "    time.sleep(3)\n",
        "    plt.savefig(plot_filename, dpi=300, bbox_inches='tight')\n",
        "    time.sleep(3)\n",
        "\n",
        "    # Save the plot to a PNG file with the updated function\n",
        "    # save_plot2png('_cruxes.png', nodate=True, base_filename=f'{PLOT_FILENAME_AUTHOR_CLEAN}_all')\n",
        "\n",
        "    # Display the plot\n",
        "    plt.show()\n",
        "\n",
        "    # Close the plot to free up memory\n",
        "    plt.close()\n",
        "\n",
        "    # Return detected points for further analysis and the filename\n",
        "    return crux_points_dt, plot_filename"
      ],
      "metadata": {
        "id": "mt5pvt9CXjF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FUTURE: Use Colab widgets\n",
        "# print(model_dropdown.value)"
      ],
      "metadata": {
        "id": "hndd7z81dra1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Model: {model_name}\")"
      ],
      "metadata": {
        "id": "X4da3BVUL70h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if GLOBAL_WIN_FLAG == True:\n",
        "  # Use Global Window Size as Percentage\n",
        "  win_per = GLOBAL_WIN\n",
        "else:\n",
        "  # Set Custom Window Size as Percentage\n",
        "  win_per = 0.10\n",
        "\n",
        "win_size = int(win_per * vader_df.shape[0])\n",
        "\n",
        "\n",
        "# 2. Create both SMA and LOWESS smoothed versions\n",
        "# smoothed_sma = smooth_ts(sentiment_mean_ls, method='sma', smooth_param=0.1)\n",
        "# smoothed_lowess = smooth_ts(sentiment_mean_ls, method='lowess', smooth_param=0.1)\n",
        "smoothed_sma = smooth_ts(sentiment_mean_ls, method='sma', smooth_param=win_per)\n",
        "smoothed_lowess = smooth_ts(sentiment_mean_ls, method='lowess', smooth_param=win_per)\n",
        "\n",
        "# 3. Set up peak detection parameters\n",
        "peak_params = {\n",
        "    'distance': 30,\n",
        "    'prominence': 0.15\n",
        "}\n",
        "\n",
        "# 4a. Generate and save SMA plot\n",
        "crux_points_sma, sma_plot_filename = plot_cruxes(\n",
        "    model_name,\n",
        "    f\"{BOOK_TITLE_AUTHOR_CLEAN}\\n {model_name} (SMA) (win={win_per})\",\n",
        "    sentiment_mean_ls,\n",
        "    smoothed_sma,\n",
        "    smooth_algo='sma',\n",
        "    peak_algo='find_peaks',\n",
        "    peak_algo_params=peak_params,\n",
        "    SIMPLE=True\n",
        ")\n",
        "\n",
        "print(f\"SAVING: \" + f\"{BOOK_TITLE_AUTHOR_CLEAN}\\n {model_name} (SMA) (win={win_per})\")\n",
        "# Download SMA plot\n",
        "time.sleep(3)\n",
        "files.download(sma_plot_filename)\n",
        "time.sleep(3)"
      ],
      "metadata": {
        "id": "y4BnxQeRXtdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smooth_algo='sma'\n",
        "peak_algo='find_peaks'\n",
        "\n",
        "print(f'{PLOT_FILENAME_AUTHOR_CLEAN}_{model_name}_{smooth_algo}_cruxes_{peak_algo}.png')"
      ],
      "metadata": {
        "id": "WKP62ruxVXH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4b. Generate and save LOWESS plot\n",
        "crux_points_lowess, lowess_plot_filename = plot_cruxes(\n",
        "    model_name,\n",
        "    f\"{BOOK_TITLE_AUTHOR_CLEAN}\\n{model_name} (LOWESS) (win={win_per})\",\n",
        "    sentiment_mean_ls,\n",
        "    smoothed_lowess,\n",
        "    smooth_algo='lowess',\n",
        "    peak_algo='find_peaks',\n",
        "    peak_algo_params=peak_params,\n",
        "    SIMPLE=True\n",
        ")\n",
        "\n",
        "# Download LOWESS plot\n",
        "\n",
        "# Delay 3s to allow time for download if [Runtime]->[Run all]\n",
        "time.sleep(3)\n",
        "files.download(lowess_plot_filename)\n",
        "time.sleep(3)"
      ],
      "metadata": {
        "id": "v33PnB3XAa0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8CwirVarBix"
      },
      "source": [
        "## Generate Crux Reports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CONFIGURE: Default length of context Window around center Crux Sentence (# sentences incl. center crux sentence)\n",
        "\n",
        "CONTEXT_WIN_DEFAULT = 11"
      ],
      "metadata": {
        "id": "cDVYR6VX9xSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_crux_text(df, smoothed_series, peak_algo='find_peaks', peak_algo_params=None, context_win=CONTEXT_WIN_DEFAULT):\n",
        "    \"\"\"\n",
        "    Extract text surrounding crux points with sentiment values. Center line is uppercased.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Input dataframe\n",
        "        smoothed_series (list): Smoothed time series\n",
        "        peak_algo (str): Peak detection algorithm\n",
        "        peak_algo_params (dict): Parameters for peak detection\n",
        "        context_win (int): Window size for text extraction (odd number)\n",
        "\n",
        "    Returns:\n",
        "        tuple: Dictionaries for peaks and valleys text with sentiment\n",
        "    \"\"\"\n",
        "    # Use detect_peaks_valleys instead of get_crux_points\n",
        "    crux_points_dt = detect_peaks_valleys(smoothed_series, peak_algo, peak_algo_params)\n",
        "\n",
        "    half_win = context_win // 2\n",
        "    peaks_dict = {}\n",
        "    valleys_dict = {}\n",
        "\n",
        "    def extract_window_text(center_idx, sentiment_val):\n",
        "        start_idx = max(0, center_idx - half_win)\n",
        "        end_idx = min(len(df), center_idx + half_win + 1)\n",
        "\n",
        "        text_lines = df['line'].iloc[start_idx:end_idx].tolist()\n",
        "        center_relative_idx = center_idx - start_idx\n",
        "        if 0 <= center_relative_idx < len(text_lines):\n",
        "            text_lines[center_relative_idx] = text_lines[center_relative_idx].upper()\n",
        "\n",
        "        return [sentiment_val, text_lines]\n",
        "\n",
        "    for peak in crux_points_dt['peaks']:\n",
        "        sentiment_val = smoothed_series[peak]\n",
        "        peaks_dict[peak] = extract_window_text(peak, sentiment_val)\n",
        "\n",
        "    for valley in crux_points_dt['valleys']:\n",
        "        sentiment_val = smoothed_series[valley]\n",
        "        valleys_dict[valley] = extract_window_text(valley, sentiment_val)\n",
        "\n",
        "    return peaks_dict, valleys_dict"
      ],
      "metadata": {
        "id": "-5osxW018W2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_crux_report(book_name, smoothed_series, df, peak_algo='find_peaks', peak_algo_params=None):\n",
        "    \"\"\"\n",
        "    Generate and save formatted report of crux points analysis.\n",
        "    Uses fixed window size of 21 lines and highlights crux point lines.\n",
        "\n",
        "    Args:\n",
        "        book_name (str): Name of the book being analyzed\n",
        "        smoothed_series (list): Smoothed time series\n",
        "        df (pd.DataFrame): Input dataframe with text\n",
        "        peak_algo (str): Peak detection algorithm\n",
        "        peak_algo_params (dict): Parameters for peak detection\n",
        "    \"\"\"\n",
        "    crux_peaks_text_dt, crux_valleys_text_dt = get_crux_text(\n",
        "        df, smoothed_series, peak_algo, peak_algo_params, context_win=21\n",
        "    )\n",
        "\n",
        "    # Prepare a sorted list of all crux points with labels\n",
        "    crux_summary = []\n",
        "    for line_no, (sentiment, text) in crux_peaks_text_dt.items():\n",
        "        crux_summary.append((line_no, sentiment, \"Peak\"))\n",
        "    for line_no, (sentiment, text) in crux_valleys_text_dt.items():\n",
        "        crux_summary.append((line_no, sentiment, \"Valley\"))\n",
        "    crux_summary.sort(key=lambda x: x[0])  # Sort by line number\n",
        "\n",
        "    report = []\n",
        "\n",
        "    # Add summary of all crux points at the top\n",
        "    report.append(f\"Sentiment Analysis Crux Points Report for: {book_name}\")\n",
        "    report.append(f\"Peak Detection Algorithm: {peak_algo}\")\n",
        "    report.append(\"Window Size: 21 lines (10 before, CRUX LINE, 10 after)\")\n",
        "    report.append(\"=\" * 80)\n",
        "    report.append(\"SUMMARY OF CRUX POINTS\")\n",
        "    report.append(\"-\" * 40)\n",
        "    for line_no, sentiment, label in crux_summary:\n",
        "        report.append(f\"{label} at line {line_no} (Sentiment: {sentiment:.3f})\")\n",
        "    report.append(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "    # Add details for Peaks\n",
        "    report.append(\"EMOTIONAL PEAKS\")\n",
        "    report.append(\"-\" * 40)\n",
        "    for line_no, (sentiment, text) in sorted(crux_peaks_text_dt.items()):\n",
        "        report.append(f\"\\nPeak at line {line_no} (Sentiment: {sentiment:.3f})\")\n",
        "        report.append(\"-\" * 40)\n",
        "        wrapped_text = [textwrap.fill(line, width=70) for line in text]\n",
        "        report.extend(wrapped_text)\n",
        "        report.append(\"-\" * 40)\n",
        "\n",
        "    # Add details for Valleys\n",
        "    report.append(\"\\nEMOTIONAL VALLEYS\")\n",
        "    report.append(\"-\" * 40)\n",
        "    for line_no, (sentiment, text) in sorted(crux_valleys_text_dt.items()):\n",
        "        report.append(f\"\\nValley at line {line_no} (Sentiment: {sentiment:.3f})\")\n",
        "        report.append(\"-\" * 40)\n",
        "        wrapped_text = [textwrap.fill(line, width=70) for line in text]\n",
        "        report.extend(wrapped_text)\n",
        "        report.append(\"-\" * 40)\n",
        "\n",
        "    # Join all report lines into a single string and save to a file\n",
        "    report_text = \"\\n\".join(report)\n",
        "    report_filename = f\"{book_name}_crux_report_{peak_algo}.txt\"\n",
        "    with open(report_filename, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(report_text)\n",
        "\n",
        "    print(report_text)\n",
        "    return report_filename  # Return the filename for use in downloading\n"
      ],
      "metadata": {
        "id": "BeAGyc3fWbfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Generate reports for both smoothing methods\n",
        "from google.colab import files\n",
        "\n",
        "# 5a. SMA report\n",
        "sma_report_filename = make_crux_report(\n",
        "    BOOK_FILENAME_AUTHOR_CLEAN + \" \" + model_name + \"_sma\",\n",
        "    smoothed_sma,\n",
        "    sentiment_all_df,\n",
        "    peak_algo='find_peaks',\n",
        "    peak_algo_params=peak_params\n",
        ")\n",
        "# Download SMA report\n",
        "\n",
        "# Delay 3s to allow time for download if [Runtime]->[Run all]\n",
        "time.sleep(3)\n",
        "files.download(sma_report_filename)\n",
        "time.sleep(3)"
      ],
      "metadata": {
        "id": "AgM459gSZYZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5b. LOWESS report\n",
        "lowess_report_filename = make_crux_report(\n",
        "    BOOK_FILENAME_AUTHOR_CLEAN + \" \" + model_name + \"_lowess\",\n",
        "    smoothed_lowess,\n",
        "    sentiment_all_df,\n",
        "    peak_algo='find_peaks',\n",
        "    peak_algo_params=peak_params\n",
        ")\n",
        "# Download LOWESS report\n",
        "\n",
        "# Delay 3s to allow time for download if [Runtime]->[Run all]\n",
        "time.sleep(3)\n",
        "files.download(lowess_report_filename)\n",
        "time.sleep(3)"
      ],
      "metadata": {
        "id": "S2eCtuL-A9-c"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "lVKek_uU7NG_",
        "w6AarbSqQweP",
        "9YGBQuOQ8262",
        "nDRQt-CWditY",
        "EMhPyO0F85GJ",
        "Sw5cff-kNOBL",
        "5L8ueH7obDFk",
        "qDa43-cGOIMa"
      ],
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}